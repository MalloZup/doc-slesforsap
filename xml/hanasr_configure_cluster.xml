<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" version="5.0"
   xmlns:xi="http://www.w3.org/2001/XInclude"
   xmlns:xlink="http://www.w3.org/1999/xlink"
   xml:id="cha.s4s.configure-cluster">
 <title>Configuration of the Cluster and &hana; Database Integration</title>
 <para>
  This chapter describes the configuration of the cluster software SUSE
  Linux Enterprise High Availability Extension, which is part of the SUSE
  Linux Enterprise Server for SAP Applications, and SAP HANA Database
  Integration.
 </para>
 <sect1>
  <title>Installation</title>

  <para>
   If not already done before, install the software packages with YaST.
   Alternatively, you can install them from the command line with zypper.
   This has to be done on both nodes.
  </para>

<screen>suse01:~> zypper in -t pattern ha_sles</screen>

  <para>
   For more information, see <citetitle>Section 3.2.2 Initial Cluster Setup
   of SUSE Linux Enterprise High Availability Extension</citetitle>
  </para>

  <figure xml:id="fig.hana-sr.pattern-sleha">
   <title>The sleha Pattern in the YaST2 Installation Dialog</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="sleha_pattern_in_yast.png" width="85%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="sleha_pattern_in_yast.png" width="85%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>
 </sect1>
 <sect1>
  <title>Basic Cluster Configuration</title>

  <para>
   The first step is to setup the base cluster framework. For convenience,
   use YaST2 or the sleha-init script. It is strongly recommended to add a
   second ring, change to UCAST com- munication and adjust the timeout
   values to your environment.
  </para>
 </sect1>
 <sect1>
  <title>Initial cluster setup using sleha-init</title>

  <para>
   For more information, see <citetitle>Section 3.4 Automatic Cluster Setup
   of of SUSE Linux Enterprise High Availability Extension</citetitle>
  </para>

  <para>
   Create an initial setup, using sleha-init command and follow the dialogs.
   This is only to be done on the first cluster node.
  </para>

<screen>sleha-init</screen>

  <para>
   So far we have configured the basic cluster framework including:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     ssh keys
    </para>
   </listitem>
   <listitem>
    <para>
     csync2 to transfer configuration files
    </para>
   </listitem>
   <listitem>
    <para>
     SBD (at least one device)
    </para>
   </listitem>
   <listitem>
    <para>
     corosync (at least one ring)
    </para>
   </listitem>
   <listitem>
    <para>
     HAWK web interface
    </para>
   </listitem>
  </itemizedlist>

  <para>
   As requested by sleha-init, change the passwords of the user hacluster.
  </para>
 </sect1>
 <sect1>
  <title>Adapting the Corosysnc and sbd Configuration</title>

  <para>
   Depending on your needs, it is possible to add a second ring and also
   change to UCAST communication as described in 3.2.1 of Best Practices for
   SAP on SUSE Linux Enterprise.
  </para>

  <para>
   Stop the already running cluster by using <command>rcopenais
   stop</command>. After setup of the corosync config and sbd parameters we
   start the cluster again.
  </para>

  <sect2>
   <title>Corosync conf</title>
   <para>
    Adjust the following blocks in the file /etc/corosync/corosync.conf, see
    also example on the end of this document.
   </para>
<screen>
totem {
     ...
      interface {
            #Network Address to be bind for this interface setting
            bindnetaddr:    192.168.1.0
            member {
                  memberaddr:    192.168.1.10
            }
            member {
                  memberaddr:    192.168.1.11
            }
      #The multicast port to be used
      mcastport:  5405
      #The ringnumber assigned to this interface setting
      ringnumber:    0
      }
      #How many threads should be used to encypt and sending message.
     threads:     4
     #
     transport:   udpu
      #How many token retransmits should be attempted before forming
      # a new configuration.
     token_retransmits_before_loss_const:    10
      #To make sure the auto-generated nodeid is positive
     clear_node_high_bit:    new
     ...
}</screen>
  </sect2>

  <sect2>
   <title>Adapting sbd config</title>
   <para>
    You can skip this section, if you do not have any sbd devices, but be
    sure to implement an other supported fencing mechanism.. If you use the
    newest updates of the pacemaker packages from the SUSE
    maintenances-channels, you can also use the -P option (<guimenu>Check
    Pacemaker quorum and node health</guimenu>) , which enables the cluster
    nodes not to self-fence if SBDs are lost, but pacemaker communication is
    still available. The timeout parameter -t defines the reprobe of the SBD
    device. The timeout is defined in seconds. Also configure a watchdog for
    the sbd devices. It is highly recommended to use a watchdog. This
    protects against failures of the sbd process itself. Please refer to the
    SLES manual for setting up a watchdog.
   </para>
   <para>
    In the following, replace /dev/disk/by-id/SBDA and SBDB by your real sbd
    device names.
   </para>
<screen># /etc/sysconfig/sbd
SBD_DEVICE="/dev/disk/by-id/SBDA;/dev/disk/by-id/SBDB"
SBD_OPTS="-W -S 1 -P -t 300"</screen>
  </sect2>

  <sect2>
   <title>Verify the sbd device</title>
   <para>
    You can skip this section, if you do not have any sbd devices, but be
    sure to implement an other supported fencing mechanism.
   </para>
   <para>
    It's a good practice to check, if the sbd device could be accessed from
    both nodes and does contain valid records. Check this for all devices
    configured in /etc/sysconfig/sbd.
   </para>
<screen>suse01:~ # sbd -d /dev/disk/by-id/SBDA dump
==Dumping header on disk /dev/disk/by-id/SBDA
Header version
: 2.1
UUID
: 0f4ea13e-fab8-4147-b9b2-3cdcfff07f86
Number of slots
: 255
Sector size
: 512
Timeout (watchdog) : 20
Timeout (allocate) : 2
Timeout (loop)
: 1
Timeout (msgwait) : 40
==Header on disk /dev/disk/by-id/SBDA is dumped</screen>
   <para>
    The timeout values in our sample are only start values, which need to be
    tuned to your environment.
   </para>
   <para>
    To check the current sbd entries for the various cluster nodes you can
    use <command>sbd list</command>. If all entries are
    <literal>clear</literal>, no fencing task is marked in the sbd device.
   </para>
<screen>suse01:~ # sbd -d /dev/disk/by-id/SBDA list
0     suse01      clear
1     suse02      clear</screen>
   <para>
    For more information on SBD configuration parameters, please read
    section 17.1.3 of SUSE Linux Enterprise High Availability Extension
   </para>
   <para>
    Now it's time to restart the cluster at the first node again (rcopenais
    start).
   </para>
  </sect2>
 </sect1>
 <sect1>
  <title>Cluster configuration on the second node</title>

  <para>
   The second node of the two nodes cluster could be integrated by starting
   the command “sleha-join”. This command just asks for the IP address
   or name of the first cluster node. Than all needed configuration files
   are copied over. As a result the cluster is started on both nodes.
  </para>

<screen>sleha-join</screen>
 </sect1>
 <sect1>
  <title>Install SAPHanaSR</title>

  <para>
   Now the resource agents for controlling the SAP HANA system replication
   need to be installed at both cluster nodes.
  </para>

<screen>suse01:~> zypper install SAPHanaSR SAPHanaSR-doc</screen>
 </sect1>
 <sect1>
  <title>Check the Cluster for the first Time</title>

  <para>
   Now it's time to check and optionally start the cluster for the first
   time on both nodes.
  </para>

<screen>suse01:~ # rcopenais status
suse02:~ # rcopenais status
suse01:~ # rcopenais start
suse02:~ # rcopenais start</screen>

  <para>
   Check the cluster status with crm_mon. We use the option "-r" to see also
   resources, which are configured but stopped.
  </para>

<screen>crm_mon -r</screen>

  <para>
   The command will show the "empty" cluster and will print something like
   in the following screen output. The most interesting information for now
   is that there are two nodes in the status "online" and the message
   "partition with quorum".
  </para>

<screen>Last updated: Fri Apr 25 09:44:59 2014
Last change: Fri Apr 25 08:30:58 2014 by root via cibadmin on suse01
Stack: classic openais (with plugin)
Current DC: suse01 - partition with quorum
Version: 1.1.9-2db99f1
2 Nodes configured, 2 expected votes
6 Resources configured.
Online: [ suse01 suse02]
Full list of resources:
stonith-sbd
(stonith:external/sbd): Started suse01</screen>
 </sect1>
 <sect1>
  <title>Configure the Cluster using crmsh</title>

  <para>
   This section describes how to configure Constraints, Resources, Bootstrap
   and STONITH using the crm configure Shell command as described in section
   7.1.2 of SUSE Linux Enterprise High Availability Extension.
  </para>

  <para>
   Use the crmsh to add the objects to CRM. Copy the following examples to a
   local file, edit the file and than load the configuration to the CRM.
  </para>

<screen>suse01:~ # vi crm-fileXX
suse01:~ # crm configure load update crm-fileXX</screen>

  <sect2>
   <title>Cluster bootstrap and more</title>
   <para>
    The first example defines the cluster bootstrap options, the resource
    and operation defaults. The stonith-timeout should be greater than 1.2
    times the SBD msgwait tiemout.
   </para>
<screen>vi crm-bs.txt
# enter the following to crm-bs.txt
property $id="cib-bootstrap-options" \
no-quorum-policy="ignore" \
stonith-enabled="true" \
stonith-action="reboot" \
stonith-timeout="150s"
rsc_defaults $id="rsc-options" \
resource-stickiness="1000" \
migration-threshold="5000"
op_defaults $id="op-options" \
timeout="600"

# now we load the file to the cluster
crm configure load update crm-bs.txt</screen>
  </sect2>

  <sect2>
   <title>STONITH device</title>
   <para>
    The next configuration part defines a SBD disk STONITH resource.
   </para>
<screen>vi crm-sbd.txt
# enter the following to crm-sbd.txt


primitive stonith-sbd stonith:external/sbd \
op start interval="0" time="15" start-delay="5"


# now we load the file to the cluster
crm configure load update crm-bs.txt</screen>
   <para>
    For alternative IPMI/ILO setup see our cluster product documentation. An
    example for a IPMI STONITH resource could be found in the appendix
    (section 7.5) of this document.
   </para>
  </sect2>

  <sect2>
   <title>SAPHanaTopology</title>
   <para>
    First we define the group of resources needed, before the HANA instances
    can be started. Edit the changes in a txt file f.e. crm-saphanatop.txt
    an load these with the command: crm configure load update
    crm-saphanatop.txt
   </para>
<screen>vi crm-saphanatop.txt
# enter the following to crm-saphanatop.txt

primitive rsc_SAPHanaTopology_SLE_HDB00 ocf:suse:SAPHanaTopology \
        operations $id="rsc_sap2_SLE_HDB00-operations" \
        op monitor interval="10" timeout="600" \
        op start interval="0" timeout="600" \
        op stop interval="0" timeout="300" \
        params SID="SLE" InstanceNumber="00"
clone cln_SAPHanaTopology_SLE_HDB00 rsc_SAPHanaTopology_SLE_HDB00 \
        meta is-managed="true" clone-node-max="1" target-role="Started" inter-
leave="true"

# now we load the file to the cluster
crm configure load update crm-saphanatop.txt</screen>
   <para>
    The most important parameters here are SID and InstanceNumber, which are
    in the SAP market quite self explaining. Beside these parameters the
    timeout values or the operations (start, monitor, stop) are typical
    tuneables.
   </para>
  </sect2>

  <sect2>
   <title>SAPHana</title>
   <para>
    First we define the group of resources needed, before the HANA instances
    can be started edit the changes in a txt file f.e. crm-saphana.txt and
    load these with the command: crm configure load update crm-saphana.txt
   </para>
<screen>vi crm-saphana.txt
# enter the following to crm-saphana.txt

primitive rsc_SAPHana_SLE_HDB00 ocf:suse:SAPHana \
        operations $id="rsc_sap_SLE_HDB00-operations" \
        op start interval="0" timeout="3600" \
        op stop interval="0" timeout="3600" \
        op promote interval="0" timeout="3600" \
        op monitor interval="60" role="Master" timeout="700" \
        op monitor interval="61" role="Slave" timeout="700" \
        params SID="SLE" InstanceNumber="00" PREFER_SITE_TAKEOVER="true" \
        DUPLICATE_PRIMARY_TIMEOUT="7200" AUTOMATED_REGISTER="false"
ms msl_SAPHana_SLE_HDB00 rsc_SAPHana_SLE_HDB00 \
        meta is-managed="true" notify="true" clone-max="2" clone-node-max="1"
target-role="Started" interleave="true"

# now we load the file to the cluster
crm configure load update crm-saphana.txt</screen>
   <para>
    The most important parameters here are SID and InstanceNumber, which are
    in the SAP market quite self explaining. Beside these parameters the
    timeout values for the operations (start, promote, monitors, stop) are
    typical tuneables.
   </para>
  </sect2>

  <sect2>
   <title>The virtual IP address</title>
   <para>
    The last resource to be added to the cluster is covering the virtual IP
    address.
   </para>
<screen>vi crm-vip.txt
# enter the following to crm-vip.txt

primitive rsc_ip_SLE_HDB00 ocf:heartbeat:IPaddr2 \
        meta target-role="Started" is-managed="true" \
        operations $id="rsc_ip_SLE_HDB00-operations" \
        op monitor interval="10s" timeout="20s" \
        params ip="192.168.1.20"

# now we load the file to the cluster
crm configure load update crm-vip.txt</screen>
   <para>
    In most installations only the parameter ip needs to be set to the
    virtual ip address to be presented to the client systems.
   </para>
  </sect2>

  <sect2>
   <title>Constraints</title>
   <para>
    Two constraints are organizing the correct placement of the virtual IP
    address for the client database access and the start order between the
    two resource agents SAPHana and SAPHanaTopology.
   </para>
<screen>vi crm-cs.txt
# enter the following to crm-cs.txt

colocation col_saphana_ip_SLE_HDB00 2000: rsc_ip_SLE_HDB00:Started \
    msl_SAPHana_SLE_HDB00:Master
order ord_SAPHana_SLE_HDB00 2000: cln_SAPHanaTopology_SLE_HDB00 \
    msl_SAPHana_SLE_HDB00

# now we load the file to the cluster
crm configure load update crm-cs.txt</screen>
  </sect2>
 </sect1>
</chapter>
